# -*- coding: utf-8 -*-
"""nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1py1UBS82whn1FuEO3ve-B3cLofazlsyM
"""

# Install and Import Required Libraries
!pip install gensim
import gensim
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np

# -------- NEW CORPUS --------
sentences = [
    "Technology is evolving rapidly with advancements in artificial intelligence.",
    "Healthcare systems rely on accurate data for patient diagnosis.",
    "Sports analytics uses data to improve player performance and strategy.",
    "Artificial intelligence helps automate complex decision making.",
    "Doctors analyze medical data to detect early symptoms of diseases.",
    "Football teams use machine learning for performance prediction.",
    "Fitness tracking devices collect real time health data.",
    "Deep learning models are transforming image recognition in healthcare.",
    "Data analytics is becoming essential in modern businesses.",
    "Athletes benefit from personalized training programs based on data insights."
]

# Tokenize
tokenized_sentences = [sentence.lower().split() for sentence in sentences]

# Train Skip-Gram Model
skipgram_model = Word2Vec(
    sentences=tokenized_sentences,
    vector_size=100,
    window=3,
    min_count=1,
    sg=1,
    epochs=100
)

# Inspect Word Vectors
word = "data"
vector = skipgram_model.wv[word]
print(f"Vector size for '{word}':", len(vector))

# Similarity
similarity_score = skipgram_model.wv.similarity("data", "health")
print("Similarity between 'data' and 'health':", similarity_score)

# Most Similar Words
similar_words = skipgram_model.wv.most_similar("data", topn=5)
print("Words similar to 'data':")
for word, score in similar_words:
    print(word, ":", score)

# ---- UPDATED PAIRS ----
pairs = [
    ("data", "health"),
    ("machine", "learning"),
    ("sports", "analytics")
]

scores = [skipgram_model.wv.similarity(w1, w2) for w1, w2 in pairs]
print("Average Similarity Score:", np.mean(scores))

# Visualization using t-SNE
words = list(skipgram_model.wv.index_to_key)
vectors = np.array([skipgram_model.wv[word] for word in words])

tsne = TSNE(n_components=2, random_state=42, perplexity=5)
reduced_vectors = tsne.fit_transform(vectors)

# Plot Word Embeddings
plt.figure(figsize=(8, 6))

for i, word in enumerate(words):
    plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1])
    plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]))

plt.title("Skip-Gram Word Embedding Visualization (t-SNE)")
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
plt.grid(True)
plt.show()

"""### **Code Explanation: Word Embeddings with Word2Vec**

This code snippet demonstrates how to create and visualize word embeddings using the Word2Vec model, specifically its Skip-Gram architecture. Word embeddings are numerical representations of words that capture semantic relationships, meaning words with similar meanings will have similar numerical representations (vectors).

Let's go through each part of the code:

#### **1. Install and Import Required Libraries**

```python
!pip install gensim
import gensim
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np
```

*   **`!pip install gensim`**: This command installs the `gensim` library, which is a popular open-source library for unsupervised topic modeling and natural language processing, including implementations of Word2Vec.
    *   **Result**: You saw output like `Collecting gensim... Successfully installed gensim-4.4.0`. This confirms that the library and its dependencies (`numpy`, `scipy`, `smart_open`, `wrapt`) were downloaded and installed into your Colab environment.
*   **`import gensim`**: Imports the `gensim` library.
*   **`from gensim.models import Word2Vec`**: Specifically imports the `Word2Vec` class from `gensim.models`. This is the core class we'll use to create word embeddings.
*   **`from sklearn.metrics.pairwise import cosine_similarity`**: Imports `cosine_similarity` from `scikit-learn`. While not directly used in the final output (gensim has its own similarity function), it's a common metric for comparing vectors and might have been intended for manual checks.
*   **`from sklearn.manifold import TSNE`**: Imports `TSNE` (t-distributed Stochastic Neighbor Embedding) from `scikit-learn`. TSNE is a dimensionality reduction technique particularly well-suited for visualizing high-dimensional data (like word vectors) in 2D or 3D.
*   **`import matplotlib.pyplot as plt`**: Imports `matplotlib.pyplot`, a plotting library, commonly aliased as `plt`.
*   **`import numpy as np`**: Imports `numpy`, a fundamental library for numerical computing in Python, commonly aliased as `np`.

#### **2. Define the Corpus (Sentences)**

```python
sentences = [
    "Technology is evolving rapidly with advancements in artificial intelligence.",
    "Healthcare systems rely on accurate data for patient diagnosis.",
    # ... (other sentences)
    "Athletes benefit from personalized training programs based on data insights."
]
```

*   **`sentences`**: This is your text corpus, a list of strings, where each string represents a sentence. Word2Vec learns word relationships by analyzing the co-occurrence of words within these sentences.

#### **3. Tokenize the Sentences**

```python
tokenized_sentences = [sentence.lower().split() for sentence in sentences]
```

*   **`tokenized_sentences`**: Before training Word2Vec, text data needs to be preprocessed. This line performs two key steps for each sentence:
    *   **`.lower()`**: Converts all characters in the sentence to lowercase. This helps treat words like "Data" and "data" as the same word.
    *   **`.split()`**: Splits each sentence into a list of individual words (tokens) based on whitespace.
*   **Term**: **Tokenization** is the process of breaking down text into smaller units called tokens (usually words or phrases).
*   **Result**: `tokenized_sentences` becomes a list of lists, where each inner list contains the lowercase words of a sentence. For example, `"Technology is evolving..."` becomes `['technology', 'is', 'evolving', ...]`. This is the format `Word2Vec` expects.

#### **4. Train the Skip-Gram Model**

```python
skipgram_model = Word2Vec(
    sentences=tokenized_sentences,
    vector_size=100,
    window=3,
    min_count=1,
    sg=1,
    epochs=100
)
```

*   **`Word2Vec(...)`**: This initiates and trains the Word2Vec model.
*   **`sentences=tokenized_sentences`**: The preprocessed list of tokenized sentences is fed to the model.
*   **`vector_size=100`**: This parameter determines the dimensionality of the word vectors (embeddings). Each word will be represented by a 100-dimensional vector. Higher dimensions can capture more nuance but require more data and computation.
*   **`window=3`**: This defines the maximum distance between the current word and the word being predicted or used as context within a sentence. A window of 3 means the model considers 3 words before and 3 words after the target word.
*   **`min_count=1`**: Words that appear fewer times than this threshold across the entire corpus are ignored. Setting it to 1 means all words (even those appearing once) will be included in the vocabulary.
*   **`sg=1`**: This specifies the training algorithm. `sg=1` means Skip-Gram, while `sg=0` would mean Continuous Bag of Words (CBOW). Skip-Gram tries to predict context words given a target word.
*   **`epochs=100`**: The number of iterations (passes) over the corpus during training. More epochs can lead to better quality embeddings but also increase training time.
*   **Term**: **Word Embeddings** are dense vector representations of words. **Skip-Gram** is an architecture within Word2Vec that aims to predict context words from a target word.

#### **5. Inspect Word Vectors**

```python
word = "data"
vector = skipgram_model.wv[word]
print(f"Vector size for '{word}':", len(vector))
```

*   **`skipgram_model.wv[word]`**: This accesses the `KeyedVectors` (`wv`) component of the trained model, which stores the learned word vectors. It retrieves the numerical vector for the specified `word`.
*   **`len(vector)`**: Returns the length (dimensionality) of the retrieved vector.
*   **Result**: `Vector size for 'data': 100`. This confirms that the word "data" has been converted into a 100-dimensional numerical array, as specified by `vector_size` during training.

#### **6. Calculate Word Similarity**

```python
similarity_score = skipgram_model.wv.similarity("data", "health")
print("Similarity between 'data' and 'health':", similarity_score)
```

*   **`skipgram_model.wv.similarity("word1", "word2")`**: This method calculates the cosine similarity between the vector representations of two words. Cosine similarity measures the cosine of the angle between two vectors; a higher cosine value (closer to 1) indicates greater similarity.
*   **Result**: `Similarity between 'data' and 'health': 0.48025241`. A score of `0.48` suggests a moderate level of semantic similarity between "data" and "health" based on how they appear in the provided small corpus.

#### **7. Find Most Similar Words**

```python
similar_words = skipgram_model.wv.most_similar("data", topn=5)
print("Words similar to 'data':")
for word, score in similar_words:
    print(word, ":", score)
```

*   **`skipgram_model.wv.most_similar("word", topn=N)`**: This method returns a list of `N` words from the vocabulary that are most similar (have the highest cosine similarity) to the given word.
*   **Result**: `Words similar to 'data': benefit : 0.5446097254753113 evolving : 0.5314252972602844 for : 0.5287097096443176 to : 0.5243873000144958 on : 0.4924267530441284`. These are the words whose vectors are closest to the vector of "data" in the 100-dimensional space, along with their similarity scores. Note that with a small corpus, sometimes very common words like 'for', 'to', 'on' might appear as similar if they frequently co-occur with the target word without necessarily being semantically similar in a human sense.

#### **8. Calculate Average Similarity Score for Pairs**

```python
pairs = [
    ("data", "health"),
    ("machine", "learning"),
    ("sports", "analytics")
]

scores = [skipgram_model.wv.similarity(w1, w2) for w1, w2 in pairs]
print("Average Similarity Score:", np.mean(scores))
```

*   **`pairs`**: A list of tuples, where each tuple represents a pair of words for which to calculate similarity.
*   **`scores = [...]`**: This uses a list comprehension to calculate the similarity score for each pair in the `pairs` list.
*   **`np.mean(scores)`**: Calculates the arithmetic mean (average) of all similarity scores.
*   **Result**: `Average Similarity Score: 0.33346006`. This gives a single aggregated value representing the average semantic relatedness of the defined pairs in your specific dataset.

#### **9. Visualization using t-SNE**

```python
words = list(skipgram_model.wv.index_to_key)
vectors = np.array([skipgram_model.wv[word] for word in words])

tsne = TSNE(n_components=2, random_state=42, perplexity=5)
reduced_vectors = tsne.fit_transform(vectors)
```

*   **`words = list(skipgram_model.wv.index_to_key)`**: Retrieves all words (vocabulary) that the Word2Vec model learned.
*   **`vectors = np.array([...])`**: Creates a NumPy array where each row is the 100-dimensional vector for a corresponding word in the `words` list.
*   **`tsne = TSNE(...)`**: Initializes the t-SNE algorithm.
    *   **`n_components=2`**: Specifies that the high-dimensional word vectors should be reduced to 2 dimensions, suitable for a 2D plot.
    *   **`random_state=42`**: Sets a seed for reproducibility. t-SNE involves randomness, so setting this ensures you get the same output every time you run it.
    *   **`perplexity=5`**: A crucial parameter for t-SNE, which relates to the number of effective nearest neighbors. It balances attention between local and global aspects of the data. Lower values consider more local neighborhoods, higher values consider more global structures. For small datasets, a smaller perplexity (e.g., 5-10) is often appropriate.
*   **`reduced_vectors = tsne.fit_transform(vectors)`**: Applies the t-SNE algorithm to the `vectors` to reduce their dimensionality to 2D. `reduced_vectors` will be an array where each row now contains two coordinates (x, y) for each word.
*   **Term**: **t-SNE (t-distributed Stochastic Neighbor Embedding)** is a non-linear dimensionality reduction algorithm used for visualizing high-dimensional datasets. It maps multi-dimensional data points to a lower-dimensional space (typically 2D or 3D) so they can be plotted, attempting to preserve the structure of nearby data points and the distances between them.

#### **10. Plot Word Embeddings**

```python
plt.figure(figsize=(8, 6))

for i, word in enumerate(words):
    plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1])
    plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]))

plt.title("Skip-Gram Word Embedding Visualization (t-SNE)")
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
plt.grid(True)
plt.show()
```

*   **`plt.figure(figsize=(8, 6))`**: Creates a new figure for the plot with a specified width and height.
*   **`for i, word in enumerate(words):`**: This loop iterates through each word and its corresponding 2D coordinates in `reduced_vectors`.
    *   **`plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1])`**: Plots each word as a scatter point using its x (Dimension 1) and y (Dimension 2) coordinates.
    *   **`plt.annotate(word, (...))`**: Adds the actual word as a label next to its scatter point on the plot. This makes it possible to identify which point corresponds to which word.
*   **`plt.title(...)`, `plt.xlabel(...)`, `plt.ylabel(...)`**: Set the title and axis labels for the plot.
*   **`plt.grid(True)`**: Adds a grid to the plot for better readability.
*   **`plt.show()`**: Displays the generated plot.
*   **Result**: A scatter plot titled "Skip-Gram Word Embedding Visualization (t-SNE)" is displayed. Each point on the plot represents a word from your corpus. Words that are semantically similar (e.g., "artificial" and "intelligence") should ideally appear closer to each other on this 2D map, reflecting their proximity in the original 100-dimensional embedding space. With a small corpus like yours, distinct clusters might not be perfectly formed, but you can still observe some groupings. For example, you might see words related to "data" or "health" clustering together if they frequently co-occur.

In summary, this code provides a complete workflow from raw text to numerical word representations and their visualization, allowing you to explore the learned semantic relationships between words.
"""

